#!/bin/bash

echo "ğŸš€ Starting AI Congress Setup..."

# Install Python dependencies
echo "ğŸ“¦ Installing Python dependencies..."
pip install -r requirements.txt

# Install frontend dependencies
echo "ğŸ¨ Installing frontend dependencies..."
cd frontend && npm install && cd ..

# Function to pull a model if not already available
pull_model() {
    local model=$1
    # Check if model is already available
    if ollama list | grep -q "$model"; then
        echo "âœ… Model $model is already available"
    else
        echo "ğŸ“¥ Pulling model: $model"
        ollama pull "$model"
        if [ $? -eq 0 ]; then
            echo "âœ… Successfully pulled $model"
        else
            echo "âŒ Failed to pull $model"
        fi
    fi
}

# Function to download Whisper model for faster-whisper
setup_whisper() {
    echo "ğŸ¤ Setting up Whisper model for voice transcription..."
    # The model will be auto-downloaded on first use by faster-whisper
    # We'll trigger a test download here
    python3 -c "
try:
    from faster_whisper import WhisperModel
    print('Downloading Whisper base model...')
    model = WhisperModel('base', device='cpu', compute_type='int8')
    print('âœ… Whisper model ready')
except Exception as e:
    print(f'âš ï¸  Whisper model will be downloaded on first use: {e}')
" || echo "âš ï¸  Whisper model will be downloaded on first use"
}

# Check Ollama configuration
OLLAMA_URL=$(python -c "import yaml; config = yaml.safe_load(open('config/config.yaml')); print(config['ollama']['base_url'])")
if [[ $OLLAMA_URL == *"localhost"* ]] || [[ $OLLAMA_URL == *"127.0.0.1"* ]]; then
    echo "ğŸ  Local Ollama detected. Pulling models..."
    pull_model "phi3:3.8b"
    pull_model "mistral:7b"
    pull_model "llama3.2:3b"
    echo "ğŸ†• Pulling lightweight versions of new models..."
    pull_model "deepseek-r1:1.5b"
    pull_model "qwen3:0.6b"
    
    echo "ğŸ¨ Pulling Stable Diffusion for image generation..."
    pull_model "stable-diffusion"
    
    # Setup Whisper for voice input
    setup_whisper
else
    echo "ğŸŒ Remote Ollama detected at $OLLAMA_URL"
    echo "âš ï¸  If this is your first time, ensure models are pulled on the server:"
    echo "    Run scripts/ollama_server.sh on your Ollama server."
fi

echo "âœ… Setup complete! Run the following:"
echo "  Backend: uvicorn src.ai_congress.api.main:app --reload"
echo "  Frontend: cd frontend && npm run dev"
